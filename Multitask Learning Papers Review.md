# Multitask Learning Papers Review

|                            Title                             |                            Method                            |                Benchmark                 | Conference | Topic                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :--------------------------------------: | :--------: | ------------------------------ |
| AutoSeM: Automatic Task Selection and Mixing in Multi-Task Learning.[[paper](https://arxiv.org/abs/1904.04153)] | Two-Stage MTL pipeline; :one:Beta-Bernoulli MAB with Thompson sampling; :two: Gaussian process based Bayesian optimization framework | [GLUE](https://arxiv.org/abs/1804.07461) | NAACL 2019 | Natural Language Understanding |
|                                                              |                                                              |                                          |            |                                |
|                                                              |                                                              |                                          |            |                                |

Directions:

- Weighting Between Tasks Loss & Training Batch 
- Model Architecture

Benchmarks:

- [GLUE](https://gluebenchmark.com/): Natural Language Understanding
- [decaNLP](https://decanlp.com/): 10 NLP Tasks

### Papers

- GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. ICLR 2019. [paper](https://arxiv.org/pdf/1804.07461)

- Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics. CVPR 2018. [paper](https://arxiv.org/pdf/1705.07115)
- A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. EMNLP 2017. [paper](https://arxiv.org/pdf/1611.01587)
- Multi-task Sequence to Sequence Learning. ICLR 2016. [paper](https://arxiv.org/pdf/1511.06114)
- The natural language decathlon: Multitask learning as question answering.  arXiv 2019. [paper](https://arxiv.org/pdf/1806.08730)
- Gated multi-task network for text classification. NAACL 2018. [paper](https://www.aclweb.org/anthology/N18-2114.pdf)
- Understanding and Improving Information Transfer in Multi-Task Learning. ICLR 2020. [paper](https://openreview.net/pdf?id=SylzhkBtDB)
- Multi-Task Deep Neural Networks for Natural Language Understanding. ACL 2019. [paper](https://arxiv.org/pdf/1901.11504)
- Which Tasks Should Be Learned Together in Multi-task Learning. CVPR 2019. [paper](https://arxiv.org/pdf/1905.07553) 

**Task Relatedness**

- Exploiting task relatedness for multiple task learning.  Learning Theory and Kernel Machines 2003. [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.8481&rep=rep1&type=pdf)
- Identifying beneficial task relations for multi-task learning in deep neural networks. EACL 2017. [paper](https://www.aclweb.org/anthology/E17-2026.pdf) 